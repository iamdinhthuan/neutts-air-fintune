# âš¡ Tá»‘i Æ°u Tá»‘c Ä‘á»™ Training

HÆ°á»›ng dáº«n tÄƒng tá»‘c training sau khi Ä‘Ã£ pre-encode dataset.

## ğŸ¯ CÃ¡c Tá»‘i Æ°u ÄÃ£ ThÃªm

### 1. **TF32 (Tensor Float 32)** - Nhanh ~20%

**Cho GPU:** RTX 30xx, RTX 40xx, A100, H100

```yaml
tf32: true  # Báº­t trong config
```

**Lá»£i Ã­ch:**
- Nhanh hÆ¡n 20% so vá»›i FP32
- Äá»™ chÃ­nh xÃ¡c gáº§n nhÆ° FP32
- KhÃ´ng tá»‘n thÃªm VRAM

**Tá»± Ä‘á»™ng báº­t náº¿u GPU há»— trá»£!**

---

### 2. **Fused AdamW Optimizer** - Nhanh ~10%

```yaml
# Tá»± Ä‘á»™ng dÃ¹ng trong code
optim: "adamw_torch_fused"
```

**Lá»£i Ã­ch:**
- Optimizer nhanh hÆ¡n AdamW thÆ°á»ng
- Ãt kernel launches hÆ¡n
- Giáº£m overhead

---

### 3. **Dataloader Optimizations** - Nhanh ~15%

```yaml
dataloader_pin_memory: true      # Pin memory
dataloader_prefetch_factor: 2    # Prefetch 2 batches
```

**Lá»£i Ã­ch:**
- Pin memory: Faster CPU â†’ GPU transfer
- Prefetch: GPU khÃ´ng pháº£i Ä‘á»£i data
- Overlap data loading vá»›i training

---

### 4. **TÄƒng Batch Size** - Nhanh ~30%

```yaml
per_device_train_batch_size: 4   # TÄƒng tá»« 1 â†’ 4
gradient_accumulation_steps: 2   # Giáº£m tá»« 4 â†’ 2
# Effective batch váº«n = 8
```

**Lá»£i Ã­ch:**
- Ãt gradient updates hÆ¡n
- GPU utilization cao hÆ¡n
- Throughput tá»‘t hÆ¡n

---

### 5. **Giáº£m Eval Frequency** - Tiáº¿t kiá»‡m thá»i gian

```yaml
eval_steps: 10000  # TÄƒng tá»« 5000 â†’ 10000
```

**Lá»£i Ã­ch:**
- Eval Ã­t hÆ¡n 50%
- Tiáº¿t kiá»‡m ~5-10% thá»i gian training
- Váº«n Ä‘á»§ Ä‘á»ƒ monitor

---

### 6. **PyTorch 2.0 Compile** (TÃ¹y chá»n) - Nhanh ~10-20%

```yaml
torch_compile: true  # Cáº§n PyTorch 2.0+
```

**Lá»£i Ã­ch:**
- Compile model thÃ nh optimized code
- Nhanh hÆ¡n 10-20%
- Láº§n Ä‘áº§u cháº­m (compile), sau Ä‘Ã³ nhanh

**LÆ°u Ã½:**
- Cáº§n PyTorch 2.0+
- Láº§n Ä‘áº§u compile máº¥t ~5-10 phÃºt
- CÃ³ thá»ƒ gáº·p lá»—i vá»›i má»™t sá»‘ models

---

### 7. **Gradient Checkpointing** (Náº¿u OOM) - Trade speed for memory

```yaml
gradient_checkpointing: true  # Chá»‰ báº­t náº¿u CUDA OOM
```

**Lá»£i Ã­ch:**
- Tiáº¿t kiá»‡m VRAM ~40%
- CÃ³ thá»ƒ train batch lá»›n hÆ¡n

**NhÆ°á»£c Ä‘iá»ƒm:**
- Cháº­m hÆ¡n ~20%
- Chá»‰ dÃ¹ng khi cáº§n thiáº¿t

---

## ğŸ“Š Tá»•ng há»£p Tá»‘i Æ°u

### Config Khuyáº¿n nghá»‹ (GPU 24GB+):

```yaml
# Dataset
dataset_path: "vietnamese_dataset.pkl"  # Pre-encoded

# Training
per_device_train_batch_size: 4
gradient_accumulation_steps: 2
num_train_epochs: 3
save_steps: 5000
eval_steps: 10000

# Speed optimizations
torch_compile: false             # Báº­t náº¿u PyTorch 2.0+
gradient_checkpointing: false    # Táº¯t (Ä‘á»§ VRAM)
tf32: true                       # Báº­t (GPU Ampere+)
dataloader_pin_memory: true
dataloader_prefetch_factor: 2
```

### Config cho GPU nhá» (8-16GB):

```yaml
# Dataset
dataset_path: "vietnamese_dataset.pkl"

# Training
per_device_train_batch_size: 1   # Giáº£m batch
gradient_accumulation_steps: 8   # TÄƒng accumulation
num_train_epochs: 3
save_steps: 5000
eval_steps: 10000

# Speed optimizations
torch_compile: false
gradient_checkpointing: true     # Báº­t Ä‘á»ƒ tiáº¿t kiá»‡m VRAM
tf32: true
dataloader_pin_memory: true
dataloader_prefetch_factor: 2
```

---

## âš¡ Æ¯á»›c tÃ­nh Tá»‘c Ä‘á»™

### Baseline (On-the-fly encoding):
```
~8-9 giÃ¢y/batch
â†’ 3 epochs = ~30 ngÃ y
```

### Pre-encoded (KhÃ´ng tá»‘i Æ°u):
```
~0.8 giÃ¢y/batch
â†’ 3 epochs = ~5 ngÃ y
```

### Pre-encoded + Tá»‘i Æ°u (Config khuyáº¿n nghá»‹):
```
~0.4-0.5 giÃ¢y/batch
â†’ 3 epochs = ~2.5-3 ngÃ y

NHANH Gáº¤P 10-12 Láº¦N SO Vá»šI BASELINE!
```

---

## ğŸ”§ CÃ¡ch Sá»­ dá»¥ng

### BÆ°á»›c 1: Cáº­p nháº­t config

Má»Ÿ `finetune_vietnamese_config.yaml`, sá»­a:

```yaml
# TÄƒng batch size
per_device_train_batch_size: 4
gradient_accumulation_steps: 2

# Giáº£m eval frequency
eval_steps: 10000

# Báº­t tá»‘i Æ°u
tf32: true
dataloader_pin_memory: true
dataloader_prefetch_factor: 2
```

### BÆ°á»›c 2: Cháº¡y training

```bash
python finetune_vietnamese.py finetune_vietnamese_config.yaml
```

### BÆ°á»›c 3: Monitor

```
[7/7] Setting up training...
  Using OnTheFlyDataCollator (on-the-fly preprocessing)
  Dataloader workers: 4 (pre-encoded data)
  âœ“ TF32 enabled for faster training

============================================================
STARTING TRAINING
============================================================
Train samples: 2591598
Val samples: 13023
Batch size per device: 4
Gradient accumulation steps: 2
Effective batch size: 8
...

Step 100: loss=2.456, time=0.45s/batch  â† Nhanh!
Step 200: loss=2.234, time=0.43s/batch
```

---

## ğŸ’¡ Tips ThÃªm

### 1. Kiá»ƒm tra GPU Utilization

```bash
# Terminal khÃ¡c
watch -n 1 nvidia-smi
```

**Má»¥c tiÃªu:** GPU Util ~95-100%

Náº¿u tháº¥p:
- TÄƒng `per_device_train_batch_size`
- TÄƒng `dataloader_num_workers`
- TÄƒng `dataloader_prefetch_factor`

---

### 2. Profile Training

```python
# ThÃªm vÃ o code náº¿u cáº§n debug
import torch.profiler

with torch.profiler.profile() as prof:
    trainer.train()

print(prof.key_averages().table())
```

---

### 3. Mixed Precision Training

ÄÃ£ báº­t sáºµn vá»›i `bf16=True`:

```yaml
# Trong TrainingArguments
bf16: true  # BFloat16 (khuyáº¿n nghá»‹ cho Ampere+)
```

**Lá»£i Ã­ch:**
- Nhanh gáº¥p ~2x so vá»›i FP32
- Tiáº¿t kiá»‡m VRAM ~50%
- á»”n Ä‘á»‹nh hÆ¡n FP16

---

### 4. Disable Unnecessary Logging

```yaml
logging_steps: 100  # TÄƒng lÃªn náº¿u log quÃ¡ nhiá»u
```

---

### 5. Sá»­ dá»¥ng SSD

Äáº£m báº£o `vietnamese_dataset.pkl` trÃªn SSD, khÃ´ng pháº£i HDD:

```bash
# Kiá»ƒm tra
df -h /media/huy/data1tb/thuan/neutts-air/

# Copy sang SSD náº¿u cáº§n
cp vietnamese_dataset.pkl /path/to/ssd/
```

---

## ğŸ“ˆ Benchmark

### GPU: RTX 3090 (24GB)

| Config | Batch | Acc | Time/batch | 3 epochs |
|--------|-------|-----|------------|----------|
| On-the-fly | 2 | 4 | 8.5s | ~30 ngÃ y |
| Pre-encoded | 2 | 4 | 0.8s | ~5 ngÃ y |
| Pre-encoded + Opt | 4 | 2 | 0.45s | ~2.8 ngÃ y |
| Pre-encoded + Opt + Compile | 4 | 2 | 0.38s | ~2.3 ngÃ y |

### GPU: A100 (40GB)

| Config | Batch | Acc | Time/batch | 3 epochs |
|--------|-------|-----|------------|----------|
| Pre-encoded + Opt | 8 | 1 | 0.35s | ~2.2 ngÃ y |
| Pre-encoded + Opt + Compile | 8 | 1 | 0.28s | ~1.8 ngÃ y |

---

## âš ï¸ Troubleshooting

### CUDA OOM khi tÄƒng batch size

```yaml
# Giáº£m batch, tÄƒng accumulation
per_device_train_batch_size: 2
gradient_accumulation_steps: 4

# Hoáº·c báº­t gradient checkpointing
gradient_checkpointing: true
```

---

### torch_compile error

```yaml
# Táº¯t náº¿u gáº·p lá»—i
torch_compile: false
```

---

### Dataloader slow

```yaml
# TÄƒng workers
dataloader_num_workers: 8  # Thá»­ 4, 8, 16

# TÄƒng prefetch
dataloader_prefetch_factor: 4
```

---

## ğŸ¯ Checklist Tá»‘i Æ°u

- [ ] Pre-encode dataset (nhanh gáº¥p 10x)
- [ ] TÄƒng batch size lÃªn 4 (náº¿u GPU Ä‘á»§)
- [ ] Báº­t TF32 (GPU Ampere+)
- [ ] Báº­t pin_memory vÃ  prefetch
- [ ] Giáº£m eval_steps xuá»‘ng 10000
- [ ] DÃ¹ng fused AdamW (tá»± Ä‘á»™ng)
- [ ] Kiá»ƒm tra GPU util ~95-100%
- [ ] Dataset trÃªn SSD
- [ ] (TÃ¹y chá»n) Báº­t torch_compile

---

## ğŸš€ Káº¿t luáº­n

**Vá»›i táº¥t cáº£ tá»‘i Æ°u:**

```
Baseline (on-the-fly): ~30 ngÃ y
â†’ Pre-encoded: ~5 ngÃ y (6x nhanh hÆ¡n)
â†’ Pre-encoded + Optimized: ~2.5-3 ngÃ y (10-12x nhanh hÆ¡n!)
```

**Báº¯t Ä‘áº§u ngay:**

```bash
# 1. Cáº­p nháº­t config
vim finetune_vietnamese_config.yaml

# 2. Train
python finetune_vietnamese.py finetune_vietnamese_config.yaml
```

**Happy fast training!** âš¡

