# ğŸš€ HÆ°á»›ng dáº«n Training Nhanh vá»›i Pre-encoded Dataset

## Táº¡i sao Pre-encode?

### âŒ On-the-fly encoding (Hiá»‡n táº¡i):
```
Má»—i batch (8 samples):
- Load 8 audio files: ~2 giÃ¢y
- Encode vá»›i NeuCodec: ~6 giÃ¢y
- Phonemize text: ~0.5 giÃ¢y
â†’ Total: ~8-9 giÃ¢y/batch

Vá»›i 2.6M samples, 3 epochs:
â†’ ~30 ngÃ y training
```

### âœ… Pre-encoded dataset (Khuyáº¿n nghá»‹):
```
Pre-encode 1 láº§n (36-48 giá»):
- Encode toÃ n bá»™ 2.6M samples
- LÆ°u vÃ o file .pkl

Training (chá»‰ load data):
- Load 8 samples tá»« RAM: ~0.1 giÃ¢y
- Phonemize text: ~0.5 giÃ¢y
â†’ Total: ~0.6 giÃ¢y/batch

Vá»›i 2.6M samples, 3 epochs:
â†’ ~3-5 ngÃ y training (nhanh gáº¥p 6-10 láº§n!)
```

---

## ğŸ“‹ BÆ¯á»šC 1: Pre-encode Dataset

### Chuáº©n bá»‹

Äáº£m báº£o báº¡n cÃ³:
- âœ… `metadata.csv` vá»›i format: `audio|transcript`
- âœ… ThÆ° má»¥c `wavs/` chá»©a audio files
- âœ… Äá»§ disk space: ~10-20GB cho 2.6M samples

### Cháº¡y Pre-encoding

```bash
# CÃ¡ch 1: DÃ¹ng Ä‘Æ°á»ng dáº«n Ä‘áº§y Ä‘á»§ (Khuyáº¿n nghá»‹)
python prepare_vietnamese_dataset.py \
    --metadata "/media/huy/data1tb/thuan/dataset/metadata.csv" \
    --audio_dir "/media/huy/data1tb/thuan/dataset/wavs" \
    --output "vietnamese_dataset.pkl" \
    --device "cuda"

# CÃ¡ch 2: Náº¿u metadata.csv vÃ  wavs/ á»Ÿ cÃ¹ng thÆ° má»¥c vá»›i script
python prepare_vietnamese_dataset.py \
    --metadata "metadata.csv" \
    --audio_dir "wavs" \
    --output "vietnamese_dataset.pkl" \
    --device "cuda"
```

### Output máº«u:

```
============================================================
PREPARING VIETNAMESE DATASET FOR NEUTTS-AIR
============================================================

[1/4] Loading NeuCodec model on cuda...
âœ“ NeuCodec loaded successfully!

[2/4] Reading metadata from /media/huy/data1tb/thuan/dataset/metadata.csv...
âœ“ Found 2,604,621 samples

[3/4] Encoding audio files with NeuCodec...
Encoding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2604621/2604621 [36:24:15<00:00, 19.87it/s]
âœ“ Successfully encoded 2,604,621 samples

[4/4] Saving encoded dataset to vietnamese_dataset.pkl...
âœ“ Dataset saved successfully!

============================================================
DATASET STATISTICS
============================================================
Total samples: 2,604,621
Average codes length: 245.3
Average text length: 67.2 characters

Sample data:
  Audio: vivoice_0.wav
  Text: Xin chÃ o Viá»‡t Nam
  Codes length: 234
  First 10 codes: [12345, 23456, 34567, ...]

âœ… Dataset preparation complete!
============================================================
```

### Thá»i gian Æ°á»›c tÃ­nh:

| Samples | GPU | Thá»i gian |
|---------|-----|-----------|
| 10,000 | RTX 3090 | ~8 phÃºt |
| 100,000 | RTX 3090 | ~1.5 giá» |
| 1,000,000 | RTX 3090 | ~15 giá» |
| 2,604,621 | RTX 3090 | ~36-40 giá» |

**ğŸ’¡ Tip:** Cháº¡y qua Ä‘Ãªm hoáº·c cuá»‘i tuáº§n!

---

## ğŸ“‹ BÆ¯á»šC 2: Cáº¥u hÃ¬nh Training

Sá»­a `finetune_vietnamese_config.yaml`:

```yaml
# Vietnamese Finetuning Configuration for NeuTTS-Air

# Model settings
restore_from: "neuphonic/neutts-air"
codebook_size: 65536
max_seq_len: 2048

# Dataset Configuration
# âœ… Sá»¬ Dá»¤NG PRE-ENCODED DATASET
dataset_path: "vietnamese_dataset.pkl"  # File vá»«a táº¡o á»Ÿ BÆ°á»›c 1

# âŒ KHÃ”NG Cáº¦N audio_dir ná»¯a (Ä‘Ã£ encode rá»“i)
# audio_dir: "/media/huy/data1tb/thuan/dataset/wavs"

# KHÃ”NG Cáº¦N max_samples (dÃ¹ng toÃ n bá»™)
max_samples: null

# Training hyperparameters
lr: 0.00004
lr_scheduler_type: "cosine"
warmup_ratio: 0.05

# Training configuration
per_device_train_batch_size: 4  # TÄƒng lÃªn 4 (nhanh hÆ¡n on-the-fly)
gradient_accumulation_steps: 2   # Giáº£m xuá»‘ng 2 (effective batch váº«n = 8)
num_train_epochs: 3
logging_steps: 100
save_steps: 5000
eval_steps: 5000

# Output
save_root: "./checkpoints"
run_name: "neutts-vietnamese"

# Other
seed: 1337
```

**Thay Ä‘á»•i quan trá»ng:**
- âœ… `dataset_path: "vietnamese_dataset.pkl"` (thay vÃ¬ CSV)
- âœ… `per_device_train_batch_size: 4` (tÄƒng tá»« 2 â†’ 4)
- âœ… `gradient_accumulation_steps: 2` (giáº£m tá»« 4 â†’ 2)
- âœ… XÃ³a `audio_dir` (khÃ´ng cáº§n ná»¯a)

---

## ğŸ“‹ BÆ¯á»šC 3: Training

```bash
python finetune_vietnamese.py finetune_vietnamese_config.yaml
```

### Output máº«u:

```
============================================================
FINETUNING NEUTTS-AIR FOR VIETNAMESE
============================================================

[1/6] Loading config from finetune_vietnamese_config.yaml
âœ“ Checkpoints will be saved to: ./checkpoints/neutts-vietnamese

[2/6] Loading model and tokenizer from neuphonic/neutts-air
âœ“ Tokenizer loaded
âœ“ Model loaded: 494,033,920 parameters

[3/6] Initializing Vietnamese phonemizer...
âœ“ Phonemizer initialized successfully!
  Test: 'Xin chÃ o' â†’ s i n tÉ• a Ë w

[4/6] Loading Vietnamese dataset...
  Loading pre-encoded dataset from vietnamese_dataset.pkl
âœ“ Loaded 2,604,621 pre-encoded samples
  Dataset columns: ['audio_file', 'text', 'codes']

[5/6] Preprocessing dataset...
  ğŸ“¦ USING PRE-ENCODED DATA
  Preprocessing all samples...
Preprocessing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2604621/2604621 [00:45:23<00:00, 956.12it/s]
âœ“ Dataset ready: 2,604,621 samples

[6/7] Splitting dataset into train/val...
âœ“ Train: 2,591,598 samples (99.5%)
âœ“ Val: 13,023 samples (0.5%)

[7/7] Setting up training...
  Using default data collator

============================================================
STARTING TRAINING
============================================================
Train samples: 2,591,598
Val samples: 13,023
Batch size per device: 4
Gradient accumulation steps: 2
Effective batch size: 8
Training epochs: 3
Estimated total steps: ~971,847
Learning rate: 4e-05
Save every: 5000 steps
Eval every: 5000 steps
============================================================

Step 100: loss=2.456, lr=4.2e-05
Step 200: loss=2.234, lr=4.3e-05
Step 5000: loss=1.987, eval_loss=2.012
Checkpoint saved: ./checkpoints/neutts-vietnamese/checkpoint-5000
...
```

---

## â±ï¸ So sÃ¡nh Thá»i gian

### On-the-fly encoding (CÅ©):

```
Pre-processing: 0 giá» (khÃ´ng cáº§n)
Training 3 epochs: ~30 ngÃ y
â†’ Total: ~30 ngÃ y
```

### Pre-encoded dataset (Má»›i):

```
Pre-encoding: ~36-40 giá» (1 láº§n duy nháº¥t)
Training 3 epochs: ~3-5 ngÃ y
â†’ Total: ~5-6 ngÃ y

Nhanh gáº¥p 5-6 láº§n!
```

**Náº¿u train nhiá»u láº§n (thá»­ nghiá»‡m hyperparameters):**

```
On-the-fly:
- Láº§n 1: 30 ngÃ y
- Láº§n 2: 30 ngÃ y
- Láº§n 3: 30 ngÃ y
â†’ Total: 90 ngÃ y

Pre-encoded:
- Pre-encode: 40 giá» (1 láº§n)
- Láº§n 1: 5 ngÃ y
- Láº§n 2: 5 ngÃ y
- Láº§n 3: 5 ngÃ y
â†’ Total: ~17 ngÃ y (nhanh gáº¥p 5 láº§n!)
```

---

## ğŸ’¾ Disk Space

### File sizes Æ°á»›c tÃ­nh:

```
metadata.csv: ~200 MB
wavs/ (2.6M files): ~50-100 GB
vietnamese_dataset.pkl: ~10-20 GB

Total: ~60-120 GB
```

**ğŸ’¡ Tip:** Sau khi pre-encode xong, báº¡n cÃ³ thá»ƒ:
- Giá»¯ `vietnamese_dataset.pkl` Ä‘á»ƒ training
- Backup `wavs/` ra á»• cá»©ng ngoÃ i (tiáº¿t kiá»‡m space)
- XÃ³a `wavs/` náº¿u cáº§n (nhÆ°ng khÃ´ng khuyáº¿n nghá»‹)

---

## ğŸ”§ Troubleshooting

### Lá»—i: "CUDA out of memory" khi pre-encode

```bash
# Giáº£i phÃ¡p 1: DÃ¹ng CPU (cháº­m hÆ¡n nhÆ°ng á»•n Ä‘á»‹nh)
python prepare_vietnamese_dataset.py \
    --metadata "metadata.csv" \
    --audio_dir "wavs" \
    --output "vietnamese_dataset.pkl" \
    --device "cpu"

# Giáº£i phÃ¡p 2: Pre-encode tá»«ng pháº§n
# Sá»­a prepare_vietnamese_dataset.py Ä‘á»ƒ xá»­ lÃ½ tá»«ng 100k samples
```

### Lá»—i: "File too large" khi save pickle

```python
# Náº¿u file > 4GB, dÃ¹ng protocol 4
# Sá»­a dÃ²ng 103 trong prepare_vietnamese_dataset.py:
pickle.dump(encoded_dataset, f, protocol=4)
```

### Pre-encoding bá»‹ giÃ¡n Ä‘oáº¡n

```bash
# ThÃªm checkpoint Ä‘á»ƒ resume
# TODO: TÃ´i cÃ³ thá»ƒ thÃªm tÃ­nh nÄƒng nÃ y náº¿u báº¡n cáº§n
```

---

## ğŸ“Š Workflow HoÃ n chá»‰nh

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FAST TRAINING WORKFLOW                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. Chuáº©n bá»‹ data
   - metadata.csv
   - wavs/
   â†“

2. Pre-encode (1 láº§n duy nháº¥t - 36-40 giá»)
   python prepare_vietnamese_dataset.py \
       --metadata "metadata.csv" \
       --audio_dir "wavs" \
       --output "vietnamese_dataset.pkl"
   â†“

3. Sá»­a config
   dataset_path: "vietnamese_dataset.pkl"
   per_device_train_batch_size: 4
   gradient_accumulation_steps: 2
   â†“

4. Training (3-5 ngÃ y)
   python finetune_vietnamese.py finetune_vietnamese_config.yaml
   â†“

5. Inference
   python infer_vietnamese.py --text "..." --ref_audio "..." --ref_text "..."
```

---

## âœ… Checklist

### Pre-encoding:
- [ ] CÃ³ Ä‘á»§ disk space (~20GB)
- [ ] metadata.csv vÃ  wavs/ sáºµn sÃ ng
- [ ] Cháº¡y `prepare_vietnamese_dataset.py`
- [ ] Äá»£i 36-40 giá» (hoáº·c qua Ä‘Ãªm)
- [ ] Kiá»ƒm tra file `vietnamese_dataset.pkl` Ä‘Ã£ táº¡o

### Training:
- [ ] Sá»­a config: `dataset_path: "vietnamese_dataset.pkl"`
- [ ] TÄƒng batch size: `per_device_train_batch_size: 4`
- [ ] Giáº£m accumulation: `gradient_accumulation_steps: 2`
- [ ] Cháº¡y training
- [ ] Äá»£i 3-5 ngÃ y

### Inference:
- [ ] Checkpoint Ä‘Ã£ save
- [ ] Test vá»›i `quick_infer.py`
- [ ] So sÃ¡nh cÃ¡c checkpoints khÃ¡c nhau

---

## ğŸ¯ Káº¿t luáº­n

**Pre-encode dataset = Äáº§u tÆ° 1 láº§n, lá»£i Ã­ch mÃ£i mÃ£i!**

- âœ… Nhanh gáº¥p 5-6 láº§n
- âœ… á»”n Ä‘á»‹nh hÆ¡n (khÃ´ng lo lá»—i encoding giá»¯a chá»«ng)
- âœ… Dá»… debug (data Ä‘Ã£ chuáº©n)
- âœ… Tiáº¿t kiá»‡m thá»i gian khi train nhiá»u láº§n

**Báº¯t Ä‘áº§u ngay:**

```bash
# BÆ°á»›c 1: Pre-encode
python prepare_vietnamese_dataset.py \
    --metadata "/media/huy/data1tb/thuan/dataset/metadata.csv" \
    --audio_dir "/media/huy/data1tb/thuan/dataset/wavs" \
    --output "vietnamese_dataset.pkl"

# BÆ°á»›c 2: Sá»­a config
# dataset_path: "vietnamese_dataset.pkl"

# BÆ°á»›c 3: Train
python finetune_vietnamese.py finetune_vietnamese_config.yaml
```

**Happy fast training!** ğŸš€

